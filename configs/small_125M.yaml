
# Small demo config ~125M params
inherit: "configs/base.yaml"

model:
  arch: "llama"
  hidden_size: 768
  n_layer: 12
  n_head: 12
  intermediate_size: 2048
  max_position_embeddings: 1024

train:
  block_size: 512
  train_steps: 300
  eval_every: 50
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 3.0e-4
  weight_decay: 0.1
  warmup_ratio: 0.05
  lr_scheduler_type: "cosine"
  logging_steps: 20
  save_every: 150
  output_dir: "checkpoints/small_125M"

