# Self-contained (no inherit)
seed: 42
project_name: "llm-from-scratch"
tokenizer_dir: "artifacts/tokenizer"
vocab_size: 20000
special_tokens:
  pad_token: "<|pad|>"
  bos_token: "<s>"
  eos_token: "</s>"

model:
  arch: "llama"
  hidden_size: 512
  n_layer: 8
  n_head: 8
  intermediate_size: 1536
  max_position_embeddings: 512
  rope_theta: 100000.0

train:
  block_size: 128
  train_steps: 400
  eval_every: 50
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 8
  learning_rate: 3.0e-4
  weight_decay: 0.1
  warmup_ratio: 0.05
  lr_scheduler_type: "cosine"
  logging_steps: 20
  save_every: 200
  bf16: false
  output_dir: "checkpoints/cpu_60M"
