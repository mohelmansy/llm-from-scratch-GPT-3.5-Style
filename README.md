# ğŸš€ LLM-from-Scratch: Training & Evaluating a Transformer Language Model  

![Python](https://img.shields.io/badge/Python-3.10-blue.svg) 
![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red.svg) 
![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-yellow.svg) 
![Status](https://img.shields.io/badge/Status-Demo%20Complete-green.svg)

---

## ğŸ“– Overview
This project is a **showcase** of building a **transformer-based language model (LLM)** entirely **from scratch** and training it on a **single 6GB GPU**.  

It highlights:  
- ğŸ—ï¸ **Model implementation** (decoder-only Transformer, GPT-style).  
- ğŸ›ï¸ **Training & checkpointing** with PyTorch.  
- ğŸ“Š **Evaluation** using perplexity and text generation.  
- ğŸ’¡ **Optimizations** for limited compute resources.  

> âš¡ The focus is on the **engineering pipeline**, not achieving SOTA performance â€” making it an excellent learning and demonstration project.  

---

## ğŸ¯ Key Objectives
âœ”ï¸ Develop a **scalable and reproducible** LLM pipeline.  
âœ”ï¸ Train a **100M parameter transformer** under strict VRAM constraints.  
âœ”ï¸ Showcase **perplexity evaluation** and **text generation**.   

---

## âš™ï¸ Tech Stack
- **Language**: Python 3.10  
- **Frameworks**: PyTorch, Hugging Face Transformers  
- **Hardware**: Single NVIDIA GPU (6GB VRAM)  
- **Environment**: Virtualenv (`.venv`)  

---

## ğŸ“‚ Repository Structure

llm-from-scratch/
â”‚â”€â”€ configs/ # Model & training configs
â”‚â”€â”€ data/ # Training datasets
â”‚â”€â”€ checkpoints/ # Model weights & checkpoints
â”‚â”€â”€ results/ # Evaluation outputs (ppl & generations)
â”‚â”€â”€ src/ # Source code
â”‚ â”œâ”€â”€ train.py # Training script
â”‚ â”œâ”€â”€ eval/
â”‚ â”‚ â”œâ”€â”€ perplexity.py # Perplexity evaluation
â”‚ â”‚ â””â”€â”€ generation_eval.py# Text generation
â”‚â”€â”€ README.md # Documentation


---

## ğŸ§  Model Architecture
- **Type**: Decoder-only Transformer (GPT-style)  
- **Parameters**: ~100M  
- **Layers**: 10  
- **Hidden Dim**: 640  
- **Attention Heads**: 10  
- **Context Window**: 192 tokens  
- **Optimizer**: AdamW + cosine decay  
- **Precision**: Mixed FP16 training  

---

ğŸš€ How to Run
1ï¸âƒ£ Prepare Dataset
python src/data/prepare_corpus.py --config configs/gpu_100M_6GB.yaml --text_dir data/sample_text --out_dir data/packed

2ï¸âƒ£ Train the Model
python -m src.train --config configs/gpu_100M_6GB.yaml


âœ”ï¸ Checkpoints are saved to:
checkpoints/gpu_100M_6GB

3ï¸âƒ£ Evaluate Perplexity
python src/eval/perplexity.py --ckpt_dir checkpoints/gpu_100M_6GB

4ï¸âƒ£ Generate Text
python src/eval/generation_eval.py --ckpt_dir checkpoints/gpu_100M_6GB --prompt "Explain gas turbines in simple terms."


ğŸ“‚ Generated text is saved in:
checkpoints/gpu_100M_6GB/sample_generation.txt

---

## ğŸ“Š Training Process
- **Dataset**: Demo text corpus (toy-scale)  
- **Batch Size**: 32  
- **Steps**: ~800  
- **Training Runtime**: ~23s per step on 6GB GPU  
- **Checkpointing**: automatic save to `checkpoints/`  

**Sample Training Log**:
{'train_runtime': 23.511, 'train_loss': 10.36, 'epoch': 1.0}
[SUCCESS] Training complete â†’ checkpoints/gpu_125M

---

## ğŸ“ˆ Evaluation

### ğŸ”¹ Perplexity
```bash
python src/eval/perplexity.py --ckpt_dir checkpoints/gpu_100M_6GB

[SUCCESS] Perplexity: 40097.322 on n=1 sequences

python src/eval/generation_eval.py --ckpt_dir checkpoints/gpu_100M_6GB \
    --prompt "Explain gas turbines in simple terms."

----- Generated text -----
xplain gas turbines in simple terms.

Outputs are stored in:

results/eval_perplexity.json
results/sample_generation.txt

ğŸ› ï¸ Challenges & Solutions
Challenge	Solution
ğŸš§ 6GB VRAM limit	Reduced model size, used FP16 training
âš ï¸ Unused token_type_ids	Removed from generate() kwargs
ğŸ“‰ High perplexity	Acknowledged dataset limitation; positioned as demo
ğŸš€ Future Enhancements

âœ… Train on larger open datasets (WikiText, OpenWebText).
âœ… Scale beyond 100M parameters using gradient checkpointing.
âœ… Add loss & perplexity trend plots.
âœ… Introduce fine-tuning (SFT, RLHF) for instruction tasks.
âœ… Deploy as an API service with FastAPI/Streamlit.
âœ… Key Takeaways

Even with 6GB GPU constraints, a functional LLM pipeline can be implemented.
The project demonstrates core building blocks of modern LLMs.
Serves as both an educational reference and a portfolio artifact for AI engineering roles.

ğŸ‘¨â€ğŸ’» Author
[Mohamed Elmansy]